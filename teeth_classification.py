# -*- coding: utf-8 -*-
"""Teeth Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11iF7LEHqcZdXfqef0Pw_Hx0IjtsQkvnI

# Import liberaries and functions
"""

import numpy as np
import pandas as pd
import os
import random
import zipfile
import imageio as iio
from PIL import Image
import tensorflow as tf
from PIL import Image, ImageEnhance
from skimage.util import random_noise
from collections import defaultdict
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, LeakyReLU
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.preprocessing.image import ImageDataGenerator

"""# Mount Colab"""

from google.colab import drive
drive.mount('/content/drive')

"""# [Unzip and extract dataset]"""

# def extract_zip(zip_path, extract_to):
#   with zipfile.ZipFile(zip_path, 'r') as zip_ref:
#     zip_ref.extractall(extract_to)
#   print(f"Extracted zip file to {extract_to}")

# # Define path to the zip file and extraction directory
# zip_path = '/content/drive/MyDrive/Teeth DataSet.zip'
# extract_to = '/content/drive/MyDrive'

# # Extract the zip file
# extract_zip(zip_path, extract_to)

"""# Read dataset without data generator

**Load images for dataset**
"""

def load_dataset(disease_names, path):
  dataset = []

  for disease in disease_names:
    disease_folder = os.path.join(path, disease)
    images = []

    for img_file in os.listdir(disease_folder):
      if img_file.lower().endswith(('.jpg', '.png', '.jpeg')):
        img_path = os.path.join(disease_folder, img_file)
        img = np.array(Image.open(img_path))
        dataset.append((img, disease))

  return dataset

"""**Load all datasets**"""

disease_names = ["CaS", "CoS", "Gum", "MC", "OC", "OLP", "OT"]
train_path = '/content/drive/MyDrive/Teeth_Dataset/Training'
test_path = '/content/drive/MyDrive/Teeth_Dataset/Testing'
val_path = '/content/drive/MyDrive/Teeth_Dataset/Validation'
train_dataset = load_dataset(disease_names, train_path)
test_dataset = load_dataset(disease_names, test_path)
val_dataset = load_dataset(disease_names, val_path)

"""**Count the number of images in each dataset**"""

def count_images(dataset, dataset_name, disease_names):
  class_counts = defaultdict(int)

  # Count images per class
  for _, label in dataset:
    class_counts[label] += 1

  # Print counts for each disease
  print(f"{dataset_name} Data:")
  for disease in disease_names:
    count = class_counts.get(disease, 0)
    print(f"Disease: {disease}, Number of Images: {count}")
  print()

count_images(train_dataset, "Training", disease_names)
count_images(test_dataset, "Testing", disease_names)
count_images(val_dataset, "Validation", disease_names)

"""**Dataset type**"""

print(type(train_dataset[0][1]))

"""# Read data using generator and augmentation

**Paths**
"""

training_path = "/content/drive/MyDrive/Teeth_Dataset/Training"
testing_path = "/content/drive/MyDrive/Teeth_Dataset/Testing"
validation_path = "/content/drive/MyDrive/Teeth_Dataset/Validation"

disease_classes = ["CaS", "CoS", "Gum", "MC", "OC", "OLP", "OT"]
size = 150
# Training data generator
train_datagen = ImageDataGenerator(
  rescale=1./255,
  rotation_range=20,
  width_shift_range=0.2,
  height_shift_range=0.2,
  shear_range=0.2,
  zoom_range=0.2,
  horizontal_flip=True,
  fill_mode='nearest'
)

# Testing data generator without augmentation
test_val_datagen = ImageDataGenerator(rescale=1./255)

# Load the training data
train_generator = train_datagen.flow_from_directory(
  training_path,
  target_size=(size,size),
  batch_size=32,
  class_mode='categorical',
  classes=disease_classes,
  shuffle=True
)

# Load the testing data
test_generator = test_val_datagen.flow_from_directory(
  testing_path,
  target_size=(size,size),
  batch_size=32,
  class_mode='categorical',
  classes=disease_classes
)

# Load the validation data
validation_generator = test_val_datagen.flow_from_directory(
  validation_path,
  target_size=(size,size),
  batch_size=32,
  class_mode='categorical',
  classes=disease_classes
)

# Get a batch of augmented data
train_images, train_labels = next(train_generator)
print(f'Training batch shape: {train_images.shape}')

test_images, test_labels = next(test_generator)
print(f'Testing batch shape: {test_images.shape}')

val_images, val_labels = next(validation_generator)
print(f'Validation batch shape: {val_images.shape}')

"""# Preprocessing

**View number of images in each disease**
"""

# def count_images(generator, dataset_name, disease_classes):
#   class_counts = defaultdict(int)

#   # Total number of samples
#   num_samples = generator.samples
#   num_batches = num_samples // generator.batch_size + 1

#   # Count images per class
#   for _ in range(num_batches):
#     images, labels = next(generator)
#     labels = np.argmax(labels, axis=1)  # Convert one-hot encoded labels to class indices

#     for label in labels:
#       for disease_name, index in generator.class_indices.items():
#         if index == label:
#           class_counts[disease_name] += len(labels)

#   # Print counts for each disease
#   print(f"{dataset_name} Data:")
#   for disease in disease_classes:
#     count = class_counts.get(disease, 0)
#     print(f"Disease: {disease}, Number of Images: {count}")
#   print()

# count_images(train_generator, "Training", disease_classes)
# count_images(test_generator, "Testing", disease_classes)
# count_images(validation_generator, "Validation", disease_classes)

"""**View samples from the data**"""

def plot_samples(generator, num_samples=5):
  # Get a batch of data
  images, labels = next(generator)

  # Get class labels and indices
  class_indices = generator.class_indices
  class_labels = {v: k for k, v in class_indices.items()}

  plt.figure(figsize=(15, 15))

  # Plot images for each class
  for class_name, index in class_indices.items():
    # Find images for the current class
    class_images = [images[i] for i in range(len(labels)) if np.argmax(labels[i]) == index]

    # Show a subset of images for the class
    for i in range(min(num_samples, len(class_images))):
      plt.subplot(len(class_indices), num_samples, index * num_samples + i + 1)
      plt.imshow(class_images[i])
      plt.axis('off')
      if i == 0:
        plt.title(class_name)

  plt.show()

plot_samples(train_generator)

"""# Preprocessing

**Min-Max Normalization**
"""

def MinMax_Nomralization(training_data):
  tf.cast(training_data[0]["images"][0], tf.float32) / 255.0

"""**Display normailzed values**"""

print(train_images[0])

"""# Model"""

model = Sequential([
  Conv2D(32, (5, 5), activation='relu', input_shape=(size, size, 3), padding='same'),
  MaxPooling2D(pool_size=(2, 2)),

  Conv2D(64, (5, 5), padding='same'), LeakyReLU(alpha=0.1),
  MaxPooling2D(pool_size=(2, 2)),

  Conv2D(128, (3, 3), padding='same'), LeakyReLU(alpha=0.1),
  BatchNormalization(),
  MaxPooling2D(pool_size=(2, 2)),

  Conv2D(32, (3, 3), padding='same'), LeakyReLU(alpha=0.1),
  BatchNormalization(),
  MaxPooling2D(pool_size=(2, 2)),

  Flatten(),
  Dense(units=50, activation='relu'),
  Dropout(0.5),

  Dense(len(disease_classes), activation='softmax')
])

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

"""**Weights initialization based on class images count**"""

# Number of samples per class
class_counts = {
    'CaS': 480,
    'CoS': 450,
    'Gum': 360,
    'MC': 540,
    'OC': 324,
    'OLP': 540,
    'OT': 393
}

# Total number of samples
total_samples = sum(class_counts.values())

# Calculate class weights
class_weights = {i: total_samples / (len(class_counts) * count) for i, count in enumerate(class_counts.values())}

print(class_weights)

"""**Fit the model**"""

# Check if GPU is available
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

# Get GPU details
gpus = tf.config.list_physical_devices('GPU')
for gpu in gpus:
    print(gpu)

epochs = 25
batch_size = 32

# reduce_lr_cb = ReduceLROnPlateau(
#     monitor='val_loss',
#     factor=0.5,
#     patience=3,
#     min_lr=1e-6,
#     verbose=1
# )

callbacks = [
  ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min'),
  EarlyStopping(monitor='val_loss', patience=5, mode='min', verbose=1),
  reduce_lr_cb
]

# Fit the model with callbacks
history = model.fit(
  train_generator,
  epochs=epochs,
  validation_data=validation_generator,
  steps_per_epoch=train_generator.samples // batch_size,
  validation_steps=validation_generator.samples // batch_size,
  verbose=1,
  callbacks=callbacks,
  class_weight=class_weights
)

"""# Plot accuracy, loss and learning rate values

"""

train_accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']

train_loss = history.history['loss']
val_loss = history.history['val_loss']

learning_rate = history.history['lr']
fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(12, 10))

ax[0].set_title('Training Accuracy vs. Epochs')
ax[0].plot(train_accuracy, 'o-', label='Train Accuracy')
ax[0].plot(val_accuracy, 'o-', label='Validation Accuracy')
ax[0].set_xlabel('Epochs')
ax[0].set_ylabel('Accuracy')
ax[0].legend(loc='best')

ax[1].set_title('Training/Validation Loss vs. Epochs')
ax[1].plot(train_loss, 'o-', label='Train Loss')
ax[1].plot(val_loss, 'o-', label='Validation Loss')
ax[1].set_xlabel('Epochs')
ax[1].set_ylabel('Loss')
ax[1].legend(loc='best')

ax[2].set_title('Learning Rate vs. Epochs')
ax[2].plot(learning_rate, 'o-', label='Learning Rate')
ax[2].set_xlabel('Epochs')
ax[2].set_ylabel('Loss')
ax[2].legend(loc='best')

plt.tight_layout()
plt.show()

"""# Testing"""

predictions = model.predict(test_generator)

"""**Display predictions**"""

fig, ax = plt.subplots(nrows=2, ncols=5, figsize=(12, 10))
idx = 0

for i in range(2):
  for j in range(5):
    predicted_label = labels[np.argmax(predictions[idx])]
    ax[i, j].set_title(f"{predicted_label}")
    ax[i, j].imshow(test_generator[0][0][idx])
    ax[i, j].axis("off")
    idx += 1

plt.tight_layout()
plt.suptitle("Test Dataset Predictions", fontsize=20)
plt.show()

"""**Evaluate the model**"""

test_loss, test_accuracy = cnn_model.evaluate(test_generator, batch_size=BATCH_SIZE)

print(f"Test Loss:     {test_loss}")
print(f"Test Accuracy: {test_accuracy}")

"""# Display Confusion Matrix"""

y_pred = np.argmax(predictions, axis=1)
y_true = test_generator.classes

cf_mtx = confusion_matrix(y_true, y_pred)

group_counts = ["{0:0.0f}".format(value) for value in cf_mtx.flatten()]
group_percentages = ["{0:.2%}".format(value) for value in cf_mtx.flatten()/np.sum(cf_mtx)]
box_labels = [f"{v1}\n({v2})" for v1, v2 in zip(group_counts, group_percentages)]
box_labels = np.asarray(box_labels).reshape(6, 6)

plt.figure(figsize = (12, 10))
sns.heatmap(cf_mtx, xticklabels=labels.values(), yticklabels=labels.values(),
           cmap="YlGnBu", fmt="", annot=box_labels)
plt.xlabel('Predicted Classes')
plt.ylabel('True Classes')
plt.show()